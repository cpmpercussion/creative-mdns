{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from edward.models import Categorical, Mixture, Normal, MultivariateNormalDiag\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data():\n",
    "    \"\"\"Generating some Slightly fuzzy sine wave data.\"\"\"\n",
    "    NSAMPLE = 50000\n",
    "    print(\"Generating\",str(NSAMPLE), \"toy data samples.\")\n",
    "    t_data = np.float32(np.array(range(NSAMPLE))/10.0)\n",
    "    t_interval = t_data[1] - t_data[0]\n",
    "    t_r_data = np.random.normal(0,t_interval/20.0,size=NSAMPLE)\n",
    "    t_data = t_data + t_r_data\n",
    "    \n",
    "    r_data = np.random.normal(size=NSAMPLE)\n",
    "    x_data = np.sin(t_data) * 7.0 + r_data * 1.0\n",
    "    df = pd.DataFrame({'t':t_data, 'x':x_data})\n",
    "    \n",
    "    window = 150\n",
    "    for n in [1200,2100,3500,4650,5212,6060]:\n",
    "        print(\"Window:\", str(n),'to',str(n+window))\n",
    "        plt.plot(df[n:n+window].t, df[n:n+window].x, '.r-')\n",
    "        plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(t_data,x_data,'ro', alpha=0.3)\n",
    "    plt.show()\n",
    "    return np.array(df)\n",
    "\n",
    "perf_df = generate_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparing sequences for MDN:\n",
    "class SequenceDataLoader(object):\n",
    "    \"\"\"Manages data from a sequence and generates epochs\"\"\"\n",
    "    def __init__(self, num_steps, batch_size, corpus):\n",
    "        \"\"\"load corpus and generate examples\"\"\"\n",
    "        self.num_steps = num_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.corpus = corpus\n",
    "        self.examples = self.setup_training_examples()\n",
    "        print(\"Done initialising loader.\")\n",
    "\n",
    "    def setup_training_examples(self):\n",
    "        xs = []\n",
    "        for i in range(len(self.corpus) - self.num_steps - 1):\n",
    "            example = self.corpus[i : i + self.num_steps]\n",
    "            xs.append(example)\n",
    "        print(\"Total training examples:\", str(len(xs)))\n",
    "        return xs\n",
    "    \n",
    "    def next_epoch(self):\n",
    "        \"\"\"Return an epoch of batches of shuffled examples.\"\"\"\n",
    "        np.random.shuffle(self.examples)\n",
    "        batches = []\n",
    "        for i in range(len(self.examples) / self.batch_size):\n",
    "            batch = self.examples[i * self.batch_size : (i + 1) * self.batch_size]\n",
    "            batches.append(batch)\n",
    "        return(np.array(batches))\n",
    "\n",
    "\n",
    "NET_MODE_TRAIN = 'train'\n",
    "NET_MODE_RUN = 'run'\n",
    "MODEL_DIR = \"/Users/charles/src/mdn-experiments/\"\n",
    "LOG_PATH = \"/tmp/tensorflow/\"\n",
    "\n",
    "class TinyJamNet2D(object):\n",
    "    def __init__(self, mode = NET_MODE_TRAIN, n_hidden_units = 24, n_mixtures = 24, batch_size = 100, sequence_length = 100):\n",
    "        \"\"\"Initialise the TinyJamNet model. Use mode='run' for evaluation graph and mode='train' for training graph.\"\"\"\n",
    "        # hyperparameters\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "        self.n_rnn_layers = 3\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.st_dev = 0.5 \n",
    "        self.n_mixtures = n_mixtures # number of mixtures\n",
    "        self.n_input_units = 2 # Number of dimensions of the input (and sampled output) data\n",
    "        self.mdn_splits = 5 # (pi, sigma_1, sigma_2, mu_1, mu_2) # forget about (rho) for now.\n",
    "        self.n_output_units = n_mixtures * self.mdn_splits # KMIX * self.mdn_splits\n",
    "        self.lr = 1e-4 # could be 1e-3\n",
    "        self.lr_decay_rate = 0.9999,  # Learning rate decay per minibatch.\n",
    "        self.lr_minimum = 0.00001,  # Minimum learning rate.\n",
    "        self.grad_clip=1.0\n",
    "        self.state = None\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        self.graph = tf.get_default_graph()\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "            with tf.name_scope('input'):\n",
    "                self.x = tf.placeholder(dtype=tf.float32, shape=[self.batch_size,self.sequence_length,self.n_input_units], name=\"x\") # input\n",
    "                self.y = tf.placeholder(dtype=tf.float32, shape=[self.batch_size,self.sequence_length,self.n_input_units], name=\"y\") # target\n",
    "            \n",
    "            self.rnn_outputs, self.init_state, self.final_state = self.recurrent_network(self.x)\n",
    "            self.rnn_outputs = tf.reshape(self.rnn_outputs,[-1,self.n_hidden_units], name = \"reshape_rnn_outputs\")\n",
    "            \n",
    "            output_params = self.fully_connected_layer(self.rnn_outputs,self.n_hidden_units,self.n_output_units)\n",
    "            \n",
    "            logits, scales_1, scales_2, locs_1, locs_2 = self.split_tensor_to_mixture_parameters(output_params)\n",
    "            cat = Categorical(logits=logits)\n",
    "            locs = tf.stack([locs_1, locs_2], axis=1)\n",
    "            scales = tf.stack([scales_1, scales_2],axis=1)\n",
    "            coll = [MultivariateNormalDiag(loc=loc, scale_diag=scale) for loc, scale \n",
    "                    in zip(tf.unstack(locs,axis=-1),tf.unstack(scales,axis=-1))]\n",
    "            \n",
    "            self.mixture = Mixture(cat=cat, \n",
    "                                   components=coll, \n",
    "                                   value=tf.zeros([self.batch_size * self.sequence_length,self.n_input_units], dtype=tf.float32)\n",
    "                                  )\n",
    "            # Saver\n",
    "            self.saver = tf.train.Saver(name = \"saver\")\n",
    "            if (mode is NET_MODE_TRAIN):\n",
    "                print(\"Loading Training Operations\")\n",
    "                self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "                y_reshaped = tf.reshape(self.y,[-1,self.n_input_units], name = \"reshape_labels\")\n",
    "                self.cost = self.loss_function(self.mixture, y_reshaped)\n",
    "                optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "                gvs = optimizer.compute_gradients(self.cost)\n",
    "                g = self.grad_clip\n",
    "                capped_gvs = [(tf.clip_by_value(grad, -g, g), var) for grad, var in gvs]\n",
    "                self.train_op = optimizer.apply_gradients(gvs, global_step=self.global_step, name='train_step')\n",
    "                #self.train_op = optimizer.minimize(self.cost, name=\"train_step\")\n",
    "                self.training_state = None\n",
    "                tf.summary.scalar(\"cost_summary\", self.cost)\n",
    "            \n",
    "            if (mode is NET_MODE_RUN):\n",
    "                print(\"Loading Running Operations\")\n",
    "                self.sample = self.mixture.sample()\n",
    "            # Summaries\n",
    "            self.summaries = tf.summary.merge_all()\n",
    "\n",
    "        self.writer = tf.summary.FileWriter(LOG_PATH, graph=self.graph)\n",
    "        train_vars_count = np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()])\n",
    "\n",
    "        print(\"done initialising:\", self.model_name(), \"vars:\", str(train_vars_count))\n",
    "                    \n",
    "    def loss_function(self,mixture,Y):\n",
    "        loss = self.mixture.log_prob(Y)\n",
    "        loss = tf.negative(loss)\n",
    "        loss = tf.reduce_sum(loss)\n",
    "        return loss\n",
    "        \n",
    "    def fully_connected_layer(self, X, in_dim, out_dim):\n",
    "        with tf.name_scope('rnn_to_mdn'):\n",
    "            W = tf.Variable(tf.random_normal([in_dim,out_dim], stddev=self.st_dev, dtype=tf.float32))\n",
    "            b = tf.Variable(tf.random_normal([1,out_dim], stddev=self.st_dev, dtype=tf.float32))\n",
    "            output = tf.matmul(X,W) + b\n",
    "        return output\n",
    "    \n",
    "    def recurrent_network(self, X):\n",
    "        with tf.name_scope('recurrent_network'):\n",
    "            cells_list = [tf.contrib.rnn.LSTMCell(self.n_hidden_units,state_is_tuple=True) for _ in range(self.n_rnn_layers)]\n",
    "            cell = tf.contrib.rnn.MultiRNNCell(cells_list, state_is_tuple=True)\n",
    "            init_state = cell.zero_state(self.batch_size,tf.float32)\n",
    "            rnn_outputs, final_state = tf.nn.dynamic_rnn(\n",
    "                cell, \n",
    "                X, \n",
    "                initial_state=init_state, \n",
    "                time_major = False, \n",
    "                dtype=tf.float32, \n",
    "                scope='RNN'\n",
    "            )\n",
    "        return rnn_outputs, init_state, final_state\n",
    "        \n",
    "    def split_tensor_to_mixture_parameters(self, output):\n",
    "        # Split up the output nodes into three groups for Pis, Sigmas and Mus.\n",
    "        logits, scales_1, scales_2, locs_1, locs_2 = tf.split(value=output, num_or_size_splits=self.mdn_splits, axis=1)\n",
    "        # Transform the sigmas to e^sigma\n",
    "        scales_1 = tf.exp(scales_1)\n",
    "        scales_2 = tf.exp(scales_2)\n",
    "        # Transform the correlations to tanh(corr)\n",
    "        #corr = tf.tanh(corr)\n",
    "        return logits, scales_1, scales_2, locs_1, locs_2\n",
    "    \n",
    "    def model_name(self):\n",
    "        \"\"\"Returns the name of the present model for saving to disk\"\"\"\n",
    "        return \"tiny-perf-mdn-\" + str(self.n_rnn_layers) + \"layers-\" + str(self.n_hidden_units) + \"units\"\n",
    "    \n",
    "    def train_batch(self, batch, sess):\n",
    "        \"\"\"Train the network on one batch\"\"\"\n",
    "        ## batch is an array of shape (batch_size, sequence_length + 1, n_input_units)\n",
    "        batch_x = batch[:,:self.sequence_length,:]\n",
    "        batch_y = batch[:,1:,:]\n",
    "        feed = {self.x: batch_x, self.y: batch_y}\n",
    "        if self.training_state is not None:\n",
    "            feed[self.init_state] = self.training_state\n",
    "        training_loss_current, self.training_state, _ = sess.run([self.cost,self.final_state,self.train_op],feed_dict=feed)\n",
    "        return training_loss_current\n",
    "\n",
    "    def train_epoch(self, batches, sess):\n",
    "        \"\"\"Train the network on one epoch of training data.\"\"\"\n",
    "        total_training_loss = 0\n",
    "        steps = 0\n",
    "        total_steps = len(batches)\n",
    "        for b in batches:\n",
    "            training_loss = self.train_batch(b, sess)\n",
    "            steps += 1\n",
    "            total_training_loss += training_loss\n",
    "            if (steps % 10 == 0):\n",
    "                print(\"Trained batch:\", str(steps), \"of\", str(total_steps), \"loss was:\", str(training_loss))\n",
    "        return total_training_loss/steps\n",
    "    \n",
    "    def train(self, data_manager, num_epochs, saving=True):\n",
    "        \"\"\"Train the network for the a number of epochs.\"\"\"\n",
    "        # often 30\n",
    "        self.num_epochs = num_epochs\n",
    "        print(\"Going to train: \" + self.model_name())\n",
    "        start_time = time.time()\n",
    "        training_losses = []\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for i in range(num_epochs):\n",
    "                batches = data_manager.next_epoch()\n",
    "                print(\"Starting Epoch\", str(i), \"of\", str(self.num_epochs))\n",
    "                epoch_average_loss = self.train_epoch(batches,sess)\n",
    "                training_losses.append(epoch_average_loss)\n",
    "                print(\"Trained Epoch\", str(i), \"of\", str(self.num_epochs))\n",
    "                if saving:\n",
    "                    self.saver.save(sess, LOG_PATH + \"/\" + self.model_name() + \".ckpt\", i)\n",
    "            if saving:\n",
    "                self.saver.save(sess,self.model_name())\n",
    "        print(\"It took \", time.time() - start_time, \" to train the network.\")\n",
    "        return training_losses\n",
    "    \n",
    "    def prepare_model_for_running(self,sess):\n",
    "        \"\"\"Load trained model and reset RNN state.\"\"\"\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        self.saver.restore(sess, MODEL_DIR + self.model_name())\n",
    "        self.state = None\n",
    "        \n",
    "    def generate_touch(self,prev_touch,sess):\n",
    "        \"\"\"Generate prediction for a single touch.\"\"\"\n",
    "        input_touch = prev_touch.reshape([1,1,self.n_input_units]) ## Give input correct shape for one-at-a-time evaluation.\n",
    "        if self.state is not None:\n",
    "            feed = {self.x: input_touch, self.init_state: self.state}\n",
    "        else:\n",
    "            feed = {self.x: input_touch}\n",
    "        prediction, self.state = sess.run([self.sample,self.final_state],feed_dict=feed)\n",
    "        return prediction\n",
    "    \n",
    "    def generate_performance(self,first_touch,number,sess):\n",
    "        self.prepare_model_for_running(sess)\n",
    "        previous_touch = first_touch\n",
    "        performance = [previous_touch.reshape((self.n_input_units,))]\n",
    "        for i in range(number):\n",
    "            previous_touch = self.generate_touch(previous_touch,sess)\n",
    "            performance.append(previous_touch.reshape((self.n_input_units,)))\n",
    "        return np.array(performance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Test\n",
    "## Train on sequences of length 121 with batch size 100.\n",
    "def test_training():\n",
    "    x_t_log = generate_data()\n",
    "    loader = SequenceDataLoader(num_steps = 121,batch_size = 100, corpus = x_t_log)\n",
    "    net = TinyJamNet2D(mode = NET_MODE_TRAIN, n_hidden_units = 128, n_mixtures = 10, batch_size = 100, sequence_length = 120)\n",
    "    losses = net.train(loader, 30, saving=True)\n",
    "    print(losses)\n",
    "    ## Plot the losses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Evaluation Test:\n",
    "## Predict 10000 Datapoints.\n",
    "\n",
    "\n",
    "test_x_t = generate_data()\n",
    "\n",
    "\n",
    "net = TinyJamNet2D(mode = NET_MODE_RUN, n_hidden_units = 128, n_mixtures = 10, batch_size = 1, sequence_length = 1)\n",
    "first_touch = test_x_t[1000].reshape((1,1,2))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    perf = net.generate_performance(first_touch,10000,sess)\n",
    "perf_df = pd.DataFrame({'t':perf.T[0], 'x':perf.T[1]})\n",
    "perf_df['time'] = perf_df.t.cumsum()\n",
    "plt.show(perf_df.plot('time','x',kind='scatter'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Investigate Output\n",
    "window = 100\n",
    "for n in [1000,2000,3000,4000,5000,6000]:\n",
    "    print(\"Window:\", str(n),'to',str(n+window))\n",
    "    plt.plot(perf_df[n:n+window].time, perf_df[n:n+window].x, '.r-')\n",
    "    plt.show()\n",
    "\n",
    "print(perf_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Discussion\n",
    "## Losses over 30 epochs:\n",
    "losses = [22901.954809472263, 17586.698399647172, 4264.8230513147564, 1143.5517320249933, -5373.3117116905123, -10477.778792231917, -12818.44131788384, -14072.937982704745, -12256.409315974837, -14305.405835929167, -15891.284362915529, -16341.383685621391, -16777.71922749592, -15573.864470455062, -16393.092865357918, -17072.193927075488, -17344.130974551281, -17383.776837574907, -14787.659655720354, -15522.463964071618, -12819.659858948735, -16396.494901971168, -17015.207802155888, -16874.642990173586, -17077.171243047138, -17597.177961234585, -17714.277263717959, -17737.939705599743, -17962.198043639401, -18552.904021971677]\n",
    "plt.plot(losses)\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
