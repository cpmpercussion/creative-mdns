{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import musical_mdn\n",
    "import sketch_mixture\n",
    "reload(sketch_mixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = MixtureRNN()\n",
    "x_t_log = musical_mdn.generate_data()\n",
    "loader = musical_mdn.SequenceDataLoader(num_steps = 121,batch_size = 100, corpus = x_t_log)\n",
    "losses = net.train(loader, 1, saving=True)\n",
    "## Plot the losses.\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NET_MODE_TRAIN = 'train'\n",
    "NET_MODE_RUN = 'run'\n",
    "MODEL_DIR = \"/home/charles/src/mdn-experiments/\"\n",
    "LOG_PATH = \"/tmp/tensorflow/\"\n",
    "\n",
    "class MixtureRNN(object):\n",
    "    def __init__(self, mode = NET_MODE_TRAIN, n_hidden_units = 128, n_mixtures = 24, batch_size = 100, sequence_length = 120):\n",
    "        \"\"\"Initialise the TinyJamNet model. Use mode='run' for evaluation graph and mode='train' for training graph.\"\"\"\n",
    "        # hyperparameters\n",
    "        self.mode = mode\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "        self.n_rnn_layers = 3\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.st_dev = 0.5 \n",
    "        self.n_mixtures = n_mixtures # number of mixtures\n",
    "        self.n_input_units = 2 # Number of dimensions of the input (and sampled output) data\n",
    "        self.mdn_splits = 6 # (pi, sigma_1, sigma_2, mu_1, mu_2) # forget about (rho) for now.\n",
    "        self.n_output_units = n_mixtures * self.mdn_splits # KMIX * self.mdn_splits\n",
    "        self.lr = 1e-4 # could be 1e-3\n",
    "        self.lr_decay_rate = 0.9999,  # Learning rate decay per minibatch.\n",
    "        self.lr_minimum = 0.00001,  # Minimum learning rate.\n",
    "        self.grad_clip=1.0\n",
    "        self.state = None\n",
    "        self.use_input_dropout = False\n",
    "        if self.mode is NET_MODE_TRAIN:\n",
    "            self.use_input_dropout = True\n",
    "        self.dropout_prob = 0.90\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        self.graph = tf.get_default_graph()\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "            with tf.name_scope('input'):\n",
    "                self.x = tf.placeholder(dtype=tf.float32, shape=[self.batch_size,self.sequence_length,self.n_input_units], name=\"x\") # input\n",
    "                self.y = tf.placeholder(dtype=tf.float32, shape=[self.batch_size,self.sequence_length,self.n_input_units], name=\"y\") # target\n",
    "            \n",
    "            self.rnn_outputs, self.init_state, self.final_state = self.recurrent_network(self.x)\n",
    "            self.rnn_outputs = tf.reshape(self.rnn_outputs,[-1,self.n_hidden_units], name = \"reshape_rnn_outputs\")\n",
    "            \n",
    "            output_params = self.fully_connected_layer(self.rnn_outputs,self.n_hidden_units,self.n_output_units)\n",
    "            \n",
    "            pis, scales_1, scales_2, locs_1, locs_2, corr = sketch_mixture.split_tensor_to_mixture_parameters(output_params)\n",
    "            # Saver\n",
    "            self.saver = tf.train.Saver(name = \"saver\")\n",
    "            if self.mode is NET_MODE_TRAIN:\n",
    "                tf.logging.info(\"Loading Training Operations\")\n",
    "                self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "                y_reshaped = tf.reshape(self.y,[-1,self.n_input_units], name = \"reshape_labels\")\n",
    "                [y1_data, y2_data] = tf.split(y_reshaped, 2, 1)\n",
    "                loss_func = sketch_mixture.get_lossfunc(pis, locs_1, locs_2, scales_1, scales_2,  corr, y1_data, y2_data)\n",
    "                self.cost = tf.reduce_mean(loss_func)\n",
    "                optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "                gvs = optimizer.compute_gradients(self.cost)\n",
    "                g = self.grad_clip\n",
    "                capped_gvs = [(tf.clip_by_value(grad, -g, g), var) for grad, var in gvs]\n",
    "                self.train_op = optimizer.apply_gradients(gvs, global_step=self.global_step, name='train_step')\n",
    "                #self.train_op = optimizer.minimize(self.cost, global_step=self.global_step, name='train_step')\n",
    "                self.training_state = None\n",
    "                tf.summary.scalar(\"cost_summary\", self.cost)\n",
    "            \n",
    "            if self.mode is NET_MODE_RUN:\n",
    "                tf.logging.info(\"Loading Running Operations\")\n",
    "                self.sample = self.mixture.sample()\n",
    "                ## TODO: write a sketch-RNN version of the sampling function?\n",
    "            # Summaries\n",
    "            self.summaries = tf.summary.merge_all()\n",
    "\n",
    "        self.writer = tf.summary.FileWriter(LOG_PATH, graph=self.graph)\n",
    "        train_vars_count = np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()])\n",
    "        tf.logging.info(\"done initialising: %s vars: %d\", self.model_name(),train_vars_count)\n",
    "        \n",
    "    def fully_connected_layer(self, X, in_dim, out_dim):\n",
    "        with tf.name_scope('rnn_to_mdn'):\n",
    "            W = tf.Variable(tf.random_normal([in_dim,out_dim], stddev=self.st_dev, dtype=tf.float32))\n",
    "            b = tf.Variable(tf.random_normal([1,out_dim], stddev=self.st_dev, dtype=tf.float32))\n",
    "            output = tf.matmul(X,W) + b\n",
    "        return output\n",
    "    \n",
    "    def recurrent_network(self, X):\n",
    "        \"\"\" Create the RNN part of the network. \"\"\"\n",
    "        with tf.name_scope('recurrent_network'):\n",
    "            cells_list = [tf.contrib.rnn.LSTMCell(self.n_hidden_units,state_is_tuple=True) for _ in range(self.n_rnn_layers)]\n",
    "            if self.use_input_dropout:\n",
    "                cells_list = [tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=self.dropout_prob) for cell in cells_list]\n",
    "            cell = tf.contrib.rnn.MultiRNNCell(cells_list, state_is_tuple=True)\n",
    "            init_state = cell.zero_state(self.batch_size,tf.float32)\n",
    "            rnn_outputs, final_state = tf.nn.dynamic_rnn(\n",
    "                cell, \n",
    "                X, \n",
    "                initial_state=init_state, \n",
    "                time_major = False, \n",
    "                dtype=tf.float32, \n",
    "                scope='RNN'\n",
    "            )\n",
    "        return rnn_outputs, init_state, final_state\n",
    "    \n",
    "    def model_name(self):\n",
    "        \"\"\"Returns the name of the present model for saving to disk\"\"\"\n",
    "        return \"musical-mdn-\" + str(self.n_rnn_layers) + \"layers-\" + str(self.n_hidden_units) + \"units\"\n",
    "    \n",
    "    def train_batch(self, batch, sess):\n",
    "        \"\"\"Train the network on one batch\"\"\"\n",
    "        ## batch is an array of shape (batch_size, sequence_length + 1, n_input_units)\n",
    "        batch_x = batch[:,:self.sequence_length,:]\n",
    "        batch_y = batch[:,1:,:]\n",
    "        feed = {self.x: batch_x, self.y: batch_y}\n",
    "        if self.training_state is not None:\n",
    "            feed[self.init_state] = self.training_state\n",
    "        training_loss_current, self.training_state, _, summary, step = sess.run([self.cost,self.final_state,self.train_op,self.summaries,self.global_step],feed_dict=feed)\n",
    "        self.writer.add_summary(summary, step)\n",
    "        return training_loss_current, step\n",
    "\n",
    "    def train_epoch(self, batches, sess):\n",
    "        \"\"\"Train the network on one epoch of training data.\"\"\"\n",
    "        total_training_loss = 0\n",
    "        epoch_steps = 0\n",
    "        total_steps = len(batches)\n",
    "        step = 0\n",
    "        for b in batches:\n",
    "            training_loss, step = self.train_batch(b, sess)\n",
    "            epoch_steps += 1\n",
    "            total_training_loss += training_loss\n",
    "            if (epoch_steps % 100 == 0):\n",
    "                tf.logging.info(\"trained batch: %d of %d; loss was %f\", steps, total_steps,training_loss)\n",
    "        return total_training_loss/epoch_steps, step\n",
    "    \n",
    "    def train(self, data_manager, num_epochs, saving=True):\n",
    "        \"\"\"Train the network for the a number of epochs.\"\"\"\n",
    "        # often 30\n",
    "        self.num_epochs = num_epochs\n",
    "        tf.logging.info(\"going to train: %s\", self.model_name())\n",
    "        start_time = time.time()\n",
    "        training_losses = []\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for i in range(num_epochs):\n",
    "                batches = data_manager.next_epoch()\n",
    "                epoch_average_loss,Â step = self.train_epoch(batches,sess)\n",
    "                training_losses.append(epoch_average_loss)\n",
    "                tf.logging.info(\"trained epoch %d of %d\", i, self.num_epochs)\n",
    "                if saving:\n",
    "                    checkpoint_path = LOG_PATH + self.model_name() + \".ckpt\"\n",
    "                    tf.logging.info('saving model %s.', checkpoint_path)\n",
    "                    tf.logging.info('global_step %i.', self.global_step)\n",
    "                    self.saver.save(sess, checkpoint_path, global_step=step)\n",
    "            if saving:\n",
    "                tf.logging.info('saving model %s.', self.model_name())\n",
    "                self.saver.save(sess,self.model_name())\n",
    "        tf.logging.info(\"took %d to train the network\", time.time() - start_time)\n",
    "        return training_losses\n",
    "    \n",
    "    def prepare_model_for_running(self,sess):\n",
    "        \"\"\"Load trained model and reset RNN state.\"\"\"\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        self.saver.restore(sess, MODEL_DIR + self.model_name())\n",
    "        self.state = None\n",
    "        \n",
    "    def generate_touch(self,prev_touch,sess):\n",
    "        \"\"\"Generate prediction for a single touch.\"\"\"\n",
    "        input_touch = prev_touch.reshape([1,1,self.n_input_units]) ## Give input correct shape for one-at-a-time evaluation.\n",
    "        if self.state is not None:\n",
    "            feed = {self.x: input_touch, self.init_state: self.state}\n",
    "        else:\n",
    "            feed = {self.x: input_touch}\n",
    "        prediction, self.state = sess.run([self.sample,self.final_state],feed_dict=feed)\n",
    "        return prediction\n",
    "    \n",
    "    def generate_performance(self,first_touch,number,sess):\n",
    "        self.prepare_model_for_running(sess)\n",
    "        previous_touch = first_touch\n",
    "        performance = [previous_touch.reshape((self.n_input_units,))]\n",
    "        for i in range(number):\n",
    "            previous_touch = self.generate_touch(previous_touch,sess)\n",
    "            performance.append(previous_touch.reshape((self.n_input_units,)))\n",
    "        return np.array(performance)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
